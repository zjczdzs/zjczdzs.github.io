[{"categories":["分布式开发"],"content":"分布式技术学习","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"一、分布式技术体系 二、分布式协调与同步 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:0:0","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"2.1 分布式互斥 传统单机上的互斥方法，为什么不能用于分布式环境呢？因为在分布式场景下,很难保证操作的原子性。 分布式系统里，这种排他性的资源访问方式，叫作**分布式互斥（Distributed Mutual Exclusion），*而这种被互斥访问的共享资源就叫作*临界资源（Critical Resource）。 如何才能让分布式系统里的程序互斥地访问临界资源。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:0","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"霸道总裁：集中式算法 ⁉️原理： 我们引入一个协调者程序，得到一个分布式互斥算法。每个程序在需要访问临界资源时，先给协调者发送一个请求。如果当前没有程序使用这个资源，协调者直接授权请求程序访问；否则，按照先来后到的顺序为请求程序“排一个号”。如果有程序使用完资源，则通知协调者，协调者从“排号”的队列里取出排在最前面的请求，并给它发送授权消息。拿到授权消息的程序，可以直接去访问临界资源。 这个互斥算法，就是我们所说的集中式算法，也可以叫做中央服务器算法。之所以这么称呼，是因为协调者代表着集中程序或中央服务器。 集中式算法的示意图如下所示： ☎️通信成本： 从上述流程可以看出，一个程序完成一次临界资源访问，需要如下几个流程和消息交互： 向协调者发送请求授权信息，1 次消息交互； 协调者向程序发放授权信息，1 次消息交互； 程序使用完临界资源后，向协调者发送释放授权，1 次消息交互。 因此，每个程序完成一次临界资源访问，需要进行 3 次消息交互。 🎉优点：集中式算法的优点在于直观、简单、信息交互量少、易于实现，并且所有程序只需和协调者通信，程序之间无需通信。 📛缺点或局限性： 一方面，协调者会成为系统的性能瓶颈。想象一下，如果有 100 个程序要访问临界资源，那么协调者要处理 100*3=300 条消息。也就是说，协调者处理的消息数量会随着需要访问临界资源的程序数量线性增加。 另一方面，容易引发单点故障问题。协调者故障，会导致所有的程序均无法访问临界资源，导致整个系统不可用。 💲应用：zookeeper，redis 因此，在使用集中式算法的时候，一定要选择性能好、可靠性高的服务器来运行协调者。 **小结一下：**集中式算法具有简单、易于实现的特点，但可用性、性能易受协调者影响。在可靠性和性能有一定保障的情况下，比如中央服务器计算能力强、性能高、故障率低，或者中央服务器进行了主备备份，主故障后备可以立马升为主，且数据可恢复的情况下，集中式算法可以适用于比较广泛的应用场景如（HDFS）。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:1","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"民主协商：分布式算法 ⁉️原理： 当一个程序要访问临界资源时，先向系统中的其他程序发送一条请求消息，在接收到所有程序返回的同意消息后，才可以访问临界资源。其中，请求消息需要包含所请求的资源、请求者的 ID，以及发起请求的时间。 这就是民主协商法。在分布式领域中，我们称之为分布式算法，或者使用组播和逻辑时钟的算法。 ☎️通信成本： 从上述流程可以看出，一个程序完成一次临界资源的访问，需要进行如下的信息交互： 向其他 n-1 个程序发送访问临界资源的请求，总共需要 n-1 次消息交互； 需要接收到其他 n-1 个程序回复的同意消息，方可访问资源，总共需要 n-1 次消息交互。 可以看出，一个程序要成功访问临界资源，至少需要 2*(n-1) 次消息交互。假设，现在系统中的 n 个程序都要访问临界资源，则会同时产生 2n(n-1) 条消息。总结来说，在大型系统中使用分布式算法，消息数量会随着需要访问临界资源的程序数量呈指数级增加，容易导致高昂的“沟通成本”。 🎉优点：分布式算法根据“先到先得”以及“投票全票通过”的机制，让每个程序按时间顺序公平地访问资源，简单粗暴、易于实现。 📛缺点或局限性： 这个算法可用性很低，主要包括两个方面的原因： 当系统内需要访问临界资源的程序增多时，容易产生“信令风暴”，也就是程序收到的请求完全超过了自己的处理能力，而导致自己正常的业务无法开展。 一旦某一程序发生故障，无法发送同意消息，那么其他程序均处在等待回复的状态中，使得整个系统处于停滞状态，导致整个系统不可用。所以，相对于集中式算法的协调者故障，分布式算法的可用性更低。 💲应用： 分布式算法适合节点数目少且变动不频繁的系统，且由于每个程序均需通信交互，因此适合 P2P 结构的系统。比如，运行在局域网中的分布式文件系统，具有 P2P 结构的系统等。 针对可用性低的一种改进办法是，如果检测到一个程序故障，则直接忽略这个程序，无需再等待它的同意消息。这就好比在自助餐厅，一个人离开餐厅了，那你在使用咖啡机前，也无需征得他的同意。但这样的话，每个程序都需要对其他程序进行故障检测，这无疑带来了更大的复杂性。 **归纳一下：**分布式算法是一个“先到先得”和“投票全票通过”的公平访问机制，但通信成本较高，可用性也比集中式算法低，适用于临界资源使用频度较低，且系统规模较小的场景。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:2","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"轮值 CEO：令牌环算法 ⁉️原理： 程序访问临界资源问题也可按照轮值 CEO 的思路实现。 如下图所示，所有程序构成一个环结构，令牌按照顺时针（或逆时针）方向在程序之间传递，收到令牌的程序有权访问临界资源，访问完成后将令牌传送到下一个程序；若该程序不需要访问临界资源，则直接把令牌传送给下一个程序。在分布式领域，这个算法叫作令牌环算法，也可以叫作基于环的算法。为了便于理解与记忆，你完全可以把这个方法形象地理解为轮值 CEO 法。 ☎️通信成本：无。 🎉优点：在令牌环算法里单个程序具有更高的通信效率。同时，在一个周期内，每个程序都能访问到临界资源，因此令牌环算法的公平性很好。 📛缺点或局限性：对于集中式和分布式算法都存在的单点故障问题，在令牌环中，若某一个程序（例如上图的无人机 2）出现故障，则直接将令牌传递给故障程序的下一个程序（例如，上图中无人机 1 直接将令牌传送给无人机 3），从而很好地解决单点故障问题，提高系统的健壮性，带来更好的可用性。但，这就要求每个程序都要记住环中的参与者信息，这样才能知道在跳过一个参与者后令牌应该传递给谁。 💲应用：综上，令牌环算法非常适合通信模式为令牌环方式的分布式系统，例如移动自组织网络系统。一个典型的应用场景就是无人机通信。 **小结一下：**令牌环算法的公平性高，在改进单点故障后，稳定性也很高，适用于系统规模较小，并且系统中每个程序使用临界资源的频率高且使用时间比较短的场景。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:3","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"知识扩展：有适合大规模系统中的分布式互斥算法吗？ 可以看到，上面提到的集中式、分布式和令牌环 3 个互斥算法，都不适用于规模过大、节点数量过多的系统。那么，什么样的互斥算法适用于大规模系统呢？ 由于大规模系统的复杂性，我们很自然地想到要用一个相对复杂的互斥算法。时下有一个很流行的互斥算法，**两层结构的分布式令牌环算法，**把整个广域网系统中的节点组织成两层结构，可以用于节点数量较多的系统，或者是广域网系统。 每个局域网中包含若干个局部进程和一个协调进程。局部进程在逻辑上组成一个环形结构，在每个环形结构上有一个局部令牌 T 在局部进程间传递。局域网与局域网之间通过各自的协调进程进行通信，这些协调进程同样组成一个环结构，这个环就是广域网中的全局环。在这个全局环上，有一个全局令牌在多个协调进程间传递。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:4","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"总结 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:1:5","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"2.2 中心化的分布式选举（共识） 集群一般是由两个或两个以上的服务器组建而成，每个服务器都是一个节点。我们经常会听到数据库集群、管理集群等概念，也知道数据库集群提供了读写功能，管理集群提供了管理、故障恢复等功能。 为什么要有分布式选举？ 主节点，在一个分布式集群中负责对其他节点的协调和管理，也就是说，其他节点都必须听从主节点的安排。 主节点的存在，就可以保证其他节点的有序运行，以及数据库集群中的写入数据在每个节点上的一致性。这里的一致性是指，数据在每个集群节点中都是一样的，不存在不同的情况。 就应了那句话“国不可一日无君”，对应到分布式系统中就是“集群不可一刻无主”。总结来说，选举的作用就是选出一个主节点，由它来协调和管理其他节点，以保证集群有序运行和节点间数据的一致性。 目前常见的选主方法有基于序号选举的算法（ 比如，Bully 算法）、多数派算法（比如，Raft 算法、ZAB 算法）等。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:0","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"长者为大：Bully 算法 🖋简介： Bully 算法是一种霸道的集群选主算法，为什么说是霸道呢？因为它的选举原则是“长者”为大，即在所有活着的节点中，选取 ID 最大的节点作为主节点。 ⁉️原理： 在 Bully 算法中，节点的角色有两种：普通节点和主节点。初始化时，所有节点都是平等的，都是普通节点，并且都有成为主的权利。但是，当选主成功后，有且仅有一个节点成为主节点，其他所有节点都是普通节点。当且仅当主节点故障或与其他节点失去联系后，才会重新选主。 Bully 算法在选举过程中，需要用到以下 3 种消息： Election 消息，用于发起选举； Alive 消息，对 Election 消息的应答； Victory 消息，竞选成功的主节点向其他节点发送的宣誓主权的消息。 Bully 算法选举的原则是“长者为大”，意味着它的**假设条件是，集群中每个节点均知道其他节点的 ID。**在此前提下，其具体的选举过程是： 集群中每个节点判断自己的 ID 是否为当前活着的节点中 ID 最大的，如果是，则直接向其他节点发送 Victory 消息，宣誓自己的主权； 如果自己不是当前活着的节点中 ID 最大的，则向比自己 ID 大的所有节点发送 Election 消息，并等待其他节点的回复； 若本节点收到比自己 ID 小的节点发送的 Election 消息，则回复一个 Alive 消息，告知其他节点，我比你大，重新选举； 若在给定的时间范围内，本节点没有收到其他节点回复的 Alive 消息，则认为自己成为主节点，并向其他节点发送 Victory 消息，宣誓自己成为主节点；若接收到来自比自己 ID 大的节点的 Alive 消息，则等待其他节点发送 Victory 消息； ☎️通信成本：不确定，但是不少。 🎉优点：Bully 算法的选择特别霸道和简单，选举谁活着且谁的 ID 最大谁就是主节点，其他节点必须无条件服从。速度快、算法复杂度低、简单易实现。 📛缺点或局限性：需要每个节点有全局的节点信息，因此额外信息存储较多；其次，任意一个比当前主节点 ID 大的新节点或节点故障后恢复加入集群的时候，都可能会触发重新选举，成为新的主节点，如果该节点频繁退出、加入集群，就会导致频繁切主。 💲应用：目前已经有很多开源软件采用了 Bully 算法进行选主，比如 MongoDB 的副本集故障转移功能。MongoDB 的分布式选举中，采用节点的最后操作时间戳来表示 ID，时间戳最新的节点其 ID 最大，也就是说时间戳最新的、活着的节点是主节点。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:1","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"民主投票：Raft 算法的投票部分 🖋简介： Raft 算法是典型的多数派投票选举的共识算法，其选举机制与我们日常生活中的民主投票机制类似，核心思想是“少数服从多数”。也就是说，Raft 算法中，获得投票最多的节点成为主。 ⁉️原理： 采用 Raft 算法选举，集群节点的角色有 3 种： Leader，即主节点，同一时刻只有一个 Leader，负责协调和管理其他节点； Candidate，即候选者，每一个节点都可以成为 Candidate，节点在该角色下才可以被选为新的 Leader； Follower，Leader 的跟随者，不可以发起选举。 Raft 选举的流程，可以分为以下几步： 初始化时，所有节点均为 Follower 状态。 开始选主时，所有节点的状态由 Follower 转化为 Candidate，并向其他节点发送选举请求。 其他节点根据接收到的选举请求的先后顺序，回复是否同意成为主。这里需要注意的是，在每一轮选举中，一个节点只能投出一张票。 若发起选举请求的节点获得超过一半的投票，则成为主节点，其状态转化为 Leader，其他节点的状态则由 Candidate 降为 Follower。Leader 节点与 Follower 节点之间会定期发送心跳包，以检测主节点是否活着。 当 Leader 节点的任期到了，即发现其他服务器开始下一轮选主周期时，Leader 节点的状态由 Leader 降级为 Follower，进入新一轮选主。 节点的状态迁移如下所示（图中的 term 指的是选举周期）： 请注意，**每一轮选举，每个节点只能投一次票。**这种选举就类似人大代表选举，正常情况下每个人大代表都有一定的任期，任期到后会触发重新选举，且投票者只能将自己手里唯一的票投给其中一个候选者。对应到 Raft 算法中，选主是周期进行的，包括选主和任值两个时间段，选主阶段对应投票阶段，任值阶段对应节点成为主之后的任期。但也有例外的时候，如果主节点故障，会立马发起选举，重新选出一个主节点。 ☎️通信成本：通信量较大。 🎉优点：Raft 算法具有选举速度快、算法复杂度低、易于实现的优点；该算法选举稳定性比 Bully 算法好，这是因为当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点获得投票数过半，才会导致切主。 📛缺点或局限性：它要求系统内每个节点都可以相互通信，且需要获得过半的投票数才能选主成功，因此通信量大。 💲应用：Google 开源的 Kubernetes，擅长容器管理与调度，为了保证可靠性，通常会部署 3 个节点用于数据备份。这 3 个节点中，有一个会被选为主，其他节点作为备。Kubernetes 的选主采用的是开源的 etcd 组件。而，etcd 的集群管理器 etcds，是一个高可用、强一致性的服务发现存储仓库，就是采用了 Raft 算法来实现选主和一致性的。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:2","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"具有优先级的民主投票：ZAB 算法 🖋简介： ZAB（ZooKeeper Atomic Broadcast）选举算法是为 ZooKeeper 实现分布式协调功能而设计的。相较于 Raft 算法的投票机制，ZAB 算法增加了通过节点 ID 和数据 ID 作为参考进行选主，节点 ID 和数据 ID 越大，表示数据越新，优先成为主。相比较于 Raft 算法，ZAB 算法尽可能保证数据的最新性。所以，ZAB 算法可以说是对 Raft 算法的改进。 ⁉️原理： 使用 ZAB 算法选举时，集群中每个节点拥有 3 种角色： Leader，主节点； Follower，跟随者节点； Observer，观察者，无投票权。 选举过程中，集群中的节点拥有 4 个状态： Looking 状态，即选举状态。当节点处于该状态时，它会认为当前集群中没有 Leader，因此自己进入选举状态。 Leading 状态，即领导者状态，表示已经选出主，且当前节点为 Leader。 Following 状态，即跟随者状态，集群中已经选出主后，其他非主节点状态更新为 Following，表示对 Leader 的追随。 Observing 状态，即观察者状态，表示当前节点为 Observer，持观望态度，没有投票权和选举权。 投票过程中，每个节点都有一个唯一的三元组 (server_id, server_zxID, epoch)，其中 server_id 表示本节点的唯一 ID；server_zxID 表示本节点存放的数据 ID，数据 ID 越大表示数据越新，选举权重越大；epoch 表示当前选取轮数，一般用逻辑时钟表示。 ZAB 选举算法的核心是“少数服从多数，ID 大的节点优先成为主”，因此选举过程中通过 (vote_id, vote_zxID) 来表明投票给哪个节点，其中 vote_id 表示被投票节点的 ID，vote_zxID 表示被投票节点的服务器 zxID。ZAB 算法选主的原则是：server_zxID 最大者成为 Leader；若 server_zxID 相同，则 server_id 最大者成为 Leader。 接下来，我以 3 个 Server 的集群为例，此处每个 Server 代表一个节点，与你介绍 ZAB 选主的过程。 第一步：当系统刚启动时，3 个服务器当前投票均为第一轮投票，即 epoch=1，且 zxID 均为 0。此时每个服务器都推选自己，并将选票信息 \u003cepoch, vote_id, vote_zxID\u003e 广播出去。 第二步：根据判断规则，由于 3 个 Server 的 epoch、zxID 都相同，因此比较 server_id，较大者即为推选对象，因此 Server 1 和 Server 2 将 vote_id 改为 3，更新自己的投票箱并重新广播自己的投票。 第三步：此时系统内所有服务器都推选了 Server 3，因此 Server 3 当选 Leader，处于 Leading 状态，向其他服务器发送心跳包并维护连接；Server1 和 Server2 处于 Following 状态。 ☎️通信成本：采用广播方式发送信息，若节点中有 n 个节点，每个节点同时广播，则集群中信息量为 n*(n-1) 个消息 🎉优点:ZAB 算法性能高，对系统无特殊要求。该算法选举稳定性比较好，当有新节点加入或节点故障恢复后，会触发选主，但不一定会真正切主，除非新节点或故障后恢复的节点数据 ID 和节点 ID 最大，且获得投票数过半，才会导致切主。 📛缺点或局限性：容易出现广播风暴；且除了投票，还增加了对比节点 ID 和数据 ID，这就意味着还需要知道所有节点的 ID 和数据 ID，所以选举时间相对较长。 💲应用：zookeeper ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:3","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"三种算法对比 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:4","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"知识扩展： 为什么“多数派”选主算法通常采用奇数节点，而不是偶数节点呢？ 多数派选主算法的核心是少数服从多数，获得投票多的节点胜出。想象一下，如果现在采用偶数节点集群，当两个节点均获得一半投票时，到底应该选谁为主呢？ 答案是，**在这种情况下，无法选出主，必须重新投票选举。**但即使重新投票选举，两个节点拥有相同投票数的概率也会很大。因此，多数派选主算法通常采用奇数节点。 这，也是大家通常看到 ZooKeeper、 etcd、Kubernetes 等开源软件选主均采用奇数节点的一个关键原因。 脑裂 脑裂问题就是在多机热备的高可用HA系统中，当两个结点心跳突然断开，纠纷列为两个独立的个体，由于互相失去联系，都认为对方出现了故障，因此都会争抢对方的资源，这就是脑裂问题 通俗的讲，脑裂(split-brain)就是“大脑分裂”，本来一个“大脑”被拆分成两个或多个。试想，如果一个人有多个大脑，且相互独立，就会导致人体“手舞足蹈”，“不听使唤”。 产生脑裂问题的原因： 1.网络问题-\u003e网络异常问题造成集群发生物理分离，造成脑裂 2.节点负载-\u003e若master结点负载过高，可能造成master结点停止响应，从而脱离集群，集群重新选主，恢复响应后出现脑裂问题 3.Leader假死-\u003e其余的followers选举出了一个新的Leader。这时，旧的Leader复活并且仍然认为自己是Leader 解决措施： 1.集群尽量部署在同一个内网环境中，从而保证各节点通讯的可靠性 2.master结点与data结点分离，保证master结点响应能力（通过node.master ：true 与 node.data：false 来决定是否有成为master结点的资格） 3.ZooKeeper维护了一个叫epoch的变量，每当新Leader产生时，会生成一个epoch标号（标识当前属于那个Leader的统治时期），epoch是递增的，followers如果确认了新的Leader存在，知道其epoch，就会拒绝epoch小于现任leader epoch的所有请求。 4.Zookeeper默认采用的是“过半原则”。所谓的过半原则就是：在Leader选举的过程中，如果某台zkServer获得了超过半数的选票，则此zkServer就可以成为Leader了。Zookeeper集群通过过半机制，达到了要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题。 5.Quorums（法定人数）方式：比如3个节点的集群，Quorums = 2，也就是说集群可以容忍1个节点失效，这时候还能选举出1个lead，集群还可用。比如4个节点的集群，它的Quorums = 3，Quorums要超过3，相当于集群的容忍度还是1，如果2个节点失效，那么整个集群还是无效的。这是ZooKeeper防止“脑裂”默认采用的方法。 6.添加心跳线：添加心跳线。原来只有一条心跳线路，此时若断开，则接收不到心跳报告，判断对方已经死亡。若有2条心跳线路，一条断开，另一条仍然能够接收心跳报告，能保证集群服务正常运行。心跳线路之间也可以 HA（高可用），这两条心跳线路之间也可以互相检测，若一条断开，则另一条马上起作用。正常情况下，则不起作用，节约资源。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:2:5","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"2.3 去中心化的分布式共识 从本质上看，分布式选举问题，其实就是传统的分布式共识方法，主要是基于多数投票策略实现的。 如果用于分布式在线记账一致性问题中，那么记账权通常会完全掌握到主节点的手里，这使得主节点非常容易造假，且存在性能瓶颈。因此，分布式选举不适用于分布式在线记账的一致性问题。 这里所说的分布式在线记账（区块链），是指在没有集中的发行方，也就是没有银行参与的情况下，任意一台接入互联网的电脑都能参与买卖，所有看到该交易的服务器都可以记录这笔交易，并且记录信息最终都是一致的，以保证交易的准确性。而如何保证交易的一致性，就是该场景下的分布式共识问题。 介绍 3 种主流的解决分布式在线记账一致性问题的共识技术 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:0","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"POW（Proof-of-Work，工作量证明） 🖋简介： 同一轮选举中有且仅有一个节点成为主节点。同理，在分布式在线记账问题中，针对同一笔交易，有且仅有一个节点或服务器可以获得记账权，然后其他节点或服务器同意该节点或服务器的记账结果，达成一致。也就是说，分布式共识包括两个关键点，获得记账权和所有节点或服务器达成一致。 ⁉️原理： 是以每个节点或服务器的计算能力（即“算力”）来竞争记账权的机制，因此是一种使用工作量证明机制的共识算法。也就是说，谁的计算力强、工作能力强，谁获得记账权的可能性就越大。 如何体现节点的“算力”呢？每个节点都去解一道题，谁能先解决谁的能力就强。 假设每个节点会划分多个区块用于记录用户交易，PoW 算法获取记账权的原理是：利用区块的 index、前一个区块的哈希值、交易的时间戳、区块数据和 nonce 值，通过 SHA256 哈希算法计算出一个哈希值，并判断前 k 个值是否都为 0。如果不是，则递增 nonce 值，重新按照上述方法计算；如果是，则本次计算的哈希值为要解决的题目的正确答案。谁最先计算出正确答案，谁就获得这个区块的记账权。 请注意：nonce 值是用来找到一个满足哈希值的数字；k 为哈希值前导零的个数，标记了计算的难度，0 越多计算难度越大。 达成共识的过程，就是获得记账权的节点将该区块信息广播给其他节点，其他节点判断该节点找到的区块中的所有交易都是有效且之前未存在过的，则认为该区块有效，并接受该区块，达成一致。 ☎️通信成本：PoW 机制每次达成共识需要全网共同参与运算 🎉优点：PoW 通过“挖矿”的方式发行新币，把比特币分散给个人，实现了相对的公平。PoW 的容错机制，允许全网 50% 的节点出错，因此，如果要破坏系统，则需要投入极大成本（若你有全球 51% 的算力，则可尝试攻击比特币）。 📛缺点或局限性：增加了每个节点的计算量，并且如果题目过难，会导致计算时间长、资源消耗多；而如果题目过于简单，会导致大量节点同时获得记账权，冲突多。这些问题，都会增加达成共识的时间。PoW 机制的缺点也很明显，共识达成的周期长、效率低，资源消耗大。 💲应用：目前，比特币平台采用了 PoW 算法，属于区块链 1.0 阶段，其重心在于货币，比特币大约 10min 才会产生一个区块，区块的大小也只有 1MB，仅能够包含 3000～4000 笔交易，平均每秒只能够处理 5~7（个位数）笔交易。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:1","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"PoS（Proof-of-Stake，权益证明） 🖋简介： 为了解决 PoW 算法的问题，引入了 PoS 算法。 ⁉️原理： 由系统权益代替算力来决定区块记账权，拥有的权益越大获得记账权的概率就越大。 所谓的权益，就是每个节点占有货币的数量和时间，而货币就是节点所获得的奖励。PoW 算法充分利用了分布式在线记账中的奖励，鼓励“利滚利”。 在股权证明 PoS 模式下，根据你持有货币的数量和时间，给你发利息。每个币每天产生 1 币龄，比如你持有 100 个币，总共持有了 50 天，那么，你的币龄就为 5000。这个时候，如果你发现了一个 PoS 区块，你的币龄就会被减少 365。每被减少 365 币龄，你就可以从区块中获得 0.05 个币的利息 (可理解为年利率 5%)。 利息 = （5000*5% ）/365 = 0.68 个币。持币有利息。 基于 PoS 算法获得区块记账权的方法与基于 PoW 的方法类似，不同之处在于：节点计算获取记账权的方法不一样，PoW 是利用区块的 index、前一个区块的哈希值、交易的时间戳、区块数据和 nonce 值，通过 SHA256 哈希算法计算出一个哈希值，并判断前 k 个值是否都为 0，而 PoS 是根据节点拥有的股权或权益进行计算的。 ☎️通信成本：每个节点在计算自己记账权的时候，通过计算自己的股权或权益来评估，如果发现自己权益最大，则将自己的区块广播给其他节点，当然必须保证该区块的有效性。 🎉优点：PoS 将算力竞争转变成权益竞争。与 PoW 相比，PoS 不需要消耗大量的电力就能够保证区块链网络的安全性，同时也不需要在每个区块中创建新的货币来激励记账者参与当前网络的运行，这也就在一定程度上缩短了达成共识所需要的时间 📛缺点或局限性：PoS 算法中持币越多或持币越久，币龄就会越高，持币人就越容易挖到区块并得到激励，而持币少的人基本没有机会，这样整个系统的安全性实际上会被持币数量较大的一部分人掌握，容易出现垄断现象。 💲应用：以太坊平台属于区块链 2.0 阶段，在区块链 1.0 的基础上进一步强调了合约，采用了 PoS 算法。12 年发布的点点币（PPC），综合了 PoW 工作量证明及 PoS 权益证明方式，从而在安全和节能方面实现了创新。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:2","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"DPoS（Delegated Proof of Stake，委托权益证明） 🖋简介： 为了解决 PoS 算法的垄断问题，2014 年比特股（BitShares）的首席开发者丹尼尔 · 拉里默（Dan Larimer）提出了委托权益证明法，也就是 DPoS 算法。 ⁉️原理： 类似股份制公司的董事会制度，普通股民虽然拥有股权，但进不了董事会，他们可以投票选举代表（受托人）代他们做决策。DPoS 是由被社区选举的可信帐户（受托人，比如得票数排行前 101 位）来拥有记账权。 为了成为正式受托人，用户要去社区拉票，获得足够多的信任。用户根据自己持有的货币数量占总量的百分比来投票，好比公司股票机制，假设总的发行股票为 1000，现在股东 A 持股 10，那么股东 A 投票权为 10/1000=1/100。如下图所示，根据自己拥有的权益，投票选出可代表自己的受托节点，受托节点之间竞争记账权。 在 DPos 算法中，通常会选出 k(比如 101) 个受托节点，它们的权利是完全相等的。受托节点之间争取记账权也是根据算力进行竞争的。只要受托节点提供的算力不稳定，计算机宕机或者利用手中的权力作恶，随时可以被握着货币的普通节点投票踢出整个系统，而后备的受托节点可以随时顶上去。 ☎️通信成本：低 🎉优点： DPoS 是在 PoW 和 PoS 的基础上进行改进的，相比于 PoS 算法，DPoS 引入了受托人，优点主要表现在： 由投票选举出的若干信誉度更高的受托人记账，解决了所有节点均参与竞争导致消息量大、达成一致的周期长的问题。也就是说，DPoS 能耗更低，具有更快的交易速度。 每隔一定周期会调整受托人，避免受托人造假和独权。 📛缺点或局限性：由于大多数持币人通过受托人参与投票，投票的积极性并不高；且一旦出现故障节点，DPoS 无法及时做出应对，导致安全隐患。 💲应用：DPoS 在比特股和 Steem 上已运行多年，整个网络中选举出的多个节点能够在 1s 之内对 99.9% 的交易进行确认。此外，DPoS 在 EOS（Enterprise Operation System，为商用分布式应用设计的一款区块链操作系统）中也有广泛应用，被称为区块链 3.0 阶段。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:3","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"三种算法对比 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:4","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"知识扩展：一致性与共识的区别是什么？ 一致性是指，分布式系统中的多个节点之间，给定一系列的操作，在约定协议的保障下，对外界呈现的数据或状态是一致的。 共识是指，分布式系统中多个节点之间，彼此对某个状态达成一致结果的过程。 也就是说，一致性强调的是结果，共识强调的是达成一致的过程，共识算法是保障系统满足不同程度一致性的核心技术。 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:5","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"总结 ","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:3:6","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"},{"categories":["分布式开发"],"content":"2.4 分布式事务","date":"2023-06-05","objectID":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/:4:0","tags":["后端","分布式"],"title":"分布式技术基础","uri":"/%E5%88%86%E5%B8%83%E5%BC%8F%E6%8A%80%E6%9C%AF/"}]